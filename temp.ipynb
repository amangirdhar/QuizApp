{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from serpapi import GoogleSearch\n",
    "\n",
    "params = {\n",
    "  \"engine\": \"google_scholar\",\n",
    "  \"q\": \"NLP\",\n",
    "  \"api_key\": \"c8b912a9727723424bffac813a03eb897d43cee8cfac0741c3b266a6cb8bef71\"\n",
    "}\n",
    "\n",
    "search = GoogleSearch(params)\n",
    "results = search.get_dict()\n",
    "organic_results = results[\"organic_results\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'position': 0,\n",
       "  'title': 'Evaluation of NLP systems',\n",
       "  'result_id': 'PEuAxkpMucUJ',\n",
       "  'link': 'https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781444324044#page=291',\n",
       "  'snippet': '… , natural language processing is concerned with the creation of artifacts that accomplish tasks. The operative question in evaluating an NLP … Because NLP encompasses an enormous …',\n",
       "  'publication_info': {'summary': 'P Resnik, J Lin - … linguistics and natural language processing, 2010 - Wiley Online Library',\n",
       "   'authors': [{'name': 'P Resnik',\n",
       "     'link': 'https://scholar.google.com/citations?user=71BFWc0AAAAJ&hl=en&oi=sra',\n",
       "     'serpapi_scholar_link': 'https://serpapi.com/search.json?author_id=71BFWc0AAAAJ&engine=google_scholar_author&hl=en',\n",
       "     'author_id': '71BFWc0AAAAJ'},\n",
       "    {'name': 'J Lin',\n",
       "     'link': 'https://scholar.google.com/citations?user=0EWw1z8AAAAJ&hl=en&oi=sra',\n",
       "     'serpapi_scholar_link': 'https://serpapi.com/search.json?author_id=0EWw1z8AAAAJ&engine=google_scholar_author&hl=en',\n",
       "     'author_id': '0EWw1z8AAAAJ'}]},\n",
       "  'resources': [{'title': 'umd.edu',\n",
       "    'file_format': 'PDF',\n",
       "    'link': 'http://www.umiacs.umd.edu/~jbg/teaching/CMSC_773_2012/reading/evaluation.pdf'}],\n",
       "  'inline_links': {'serpapi_cite_link': 'https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=PEuAxkpMucUJ',\n",
       "   'cited_by': {'total': 115,\n",
       "    'link': 'https://scholar.google.com/scholar?cites=14247502780204862268&as_sdt=5,33&sciodt=0,33&hl=en',\n",
       "    'cites_id': '14247502780204862268',\n",
       "    'serpapi_scholar_link': 'https://serpapi.com/search.json?as_sdt=5%2C33&cites=14247502780204862268&engine=google_scholar&hl=en'},\n",
       "   'related_pages_link': 'https://scholar.google.com/scholar?q=related:PEuAxkpMucUJ:scholar.google.com/&scioq=NLP&hl=en&as_sdt=0,33',\n",
       "   'serpapi_related_pages_link': 'https://serpapi.com/search.json?as_sdt=0%2C33&engine=google_scholar&hl=en&q=related%3APEuAxkpMucUJ%3Ascholar.google.com%2F',\n",
       "   'versions': {'total': 16,\n",
       "    'link': 'https://scholar.google.com/scholar?cluster=14247502780204862268&hl=en&as_sdt=0,33',\n",
       "    'cluster_id': '14247502780204862268',\n",
       "    'serpapi_scholar_link': 'https://serpapi.com/search.json?as_sdt=0%2C33&cluster=14247502780204862268&engine=google_scholar&hl=en'}}},\n",
       " {'position': 1,\n",
       "  'title': 'Jumping NLP curves: A review of natural language processing research',\n",
       "  'result_id': 'uBbmIZ9hCmYJ',\n",
       "  'link': 'https://ieeexplore.ieee.org/abstract/document/6786458/',\n",
       "  'snippet': '… the evolution of NLP research … NLP research has been gradually shifting from lexical semantics to compositional semantics and offers insights on next-generation narrative-based NLP …',\n",
       "  'publication_info': {'summary': 'E Cambria, B White - IEEE Computational intelligence …, 2014 - ieeexplore.ieee.org',\n",
       "   'authors': [{'name': 'E Cambria',\n",
       "     'link': 'https://scholar.google.com/citations?user=ilSYpW0AAAAJ&hl=en&oi=sra',\n",
       "     'serpapi_scholar_link': 'https://serpapi.com/search.json?author_id=ilSYpW0AAAAJ&engine=google_scholar_author&hl=en',\n",
       "     'author_id': 'ilSYpW0AAAAJ'}]},\n",
       "  'resources': [{'title': 'sentic.net',\n",
       "    'file_format': 'PDF',\n",
       "    'link': 'http://www.sentic.net/jumping-nlp-curves.pdf'}],\n",
       "  'inline_links': {'serpapi_cite_link': 'https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=uBbmIZ9hCmYJ',\n",
       "   'cited_by': {'total': 1513,\n",
       "    'link': 'https://scholar.google.com/scholar?cites=7352796677732177592&as_sdt=5,33&sciodt=0,33&hl=en',\n",
       "    'cites_id': '7352796677732177592',\n",
       "    'serpapi_scholar_link': 'https://serpapi.com/search.json?as_sdt=5%2C33&cites=7352796677732177592&engine=google_scholar&hl=en'},\n",
       "   'related_pages_link': 'https://scholar.google.com/scholar?q=related:uBbmIZ9hCmYJ:scholar.google.com/&scioq=NLP&hl=en&as_sdt=0,33',\n",
       "   'serpapi_related_pages_link': 'https://serpapi.com/search.json?as_sdt=0%2C33&engine=google_scholar&hl=en&q=related%3AuBbmIZ9hCmYJ%3Ascholar.google.com%2F',\n",
       "   'versions': {'total': 16,\n",
       "    'link': 'https://scholar.google.com/scholar?cluster=7352796677732177592&hl=en&as_sdt=0,33',\n",
       "    'cluster_id': '7352796677732177592',\n",
       "    'serpapi_scholar_link': 'https://serpapi.com/search.json?as_sdt=0%2C33&cluster=7352796677732177592&engine=google_scholar&hl=en'}}},\n",
       " {'position': 2,\n",
       "  'title': 'NLP (natural language processing) for NLP (natural language programming)',\n",
       "  'result_id': 'cApB1uQI-aEJ',\n",
       "  'link': 'https://link.springer.com/chapter/10.1007/11671299_34',\n",
       "  'snippet': '… Natural Language Processing holds great promise for making computer interfaces that are … We believe that modern Natural Language Processing techniques can make possible the use …',\n",
       "  'publication_info': {'summary': 'R Mihalcea, H Liu, H Lieberman - … 2006, Mexico City, Mexico, February 19 …, 2006 - Springer',\n",
       "   'authors': [{'name': 'R Mihalcea',\n",
       "     'link': 'https://scholar.google.com/citations?user=UetM7FgAAAAJ&hl=en&oi=sra',\n",
       "     'serpapi_scholar_link': 'https://serpapi.com/search.json?author_id=UetM7FgAAAAJ&engine=google_scholar_author&hl=en',\n",
       "     'author_id': 'UetM7FgAAAAJ'},\n",
       "    {'name': 'H Liu',\n",
       "     'link': 'https://scholar.google.com/citations?user=6-YuY60AAAAJ&hl=en&oi=sra',\n",
       "     'serpapi_scholar_link': 'https://serpapi.com/search.json?author_id=6-YuY60AAAAJ&engine=google_scholar_author&hl=en',\n",
       "     'author_id': '6-YuY60AAAAJ'},\n",
       "    {'name': 'H Lieberman',\n",
       "     'link': 'https://scholar.google.com/citations?user=NdV5hMYAAAAJ&hl=en&oi=sra',\n",
       "     'serpapi_scholar_link': 'https://serpapi.com/search.json?author_id=NdV5hMYAAAAJ&engine=google_scholar_author&hl=en',\n",
       "     'author_id': 'NdV5hMYAAAAJ'}]},\n",
       "  'resources': [{'title': 'umich.edu',\n",
       "    'file_format': 'PS',\n",
       "    'link': 'https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.cicling06a.ps'}],\n",
       "  'inline_links': {'serpapi_cite_link': 'https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=cApB1uQI-aEJ',\n",
       "   'cited_by': {'total': 129,\n",
       "    'link': 'https://scholar.google.com/scholar?cites=11671369688247503472&as_sdt=5,33&sciodt=0,33&hl=en',\n",
       "    'cites_id': '11671369688247503472',\n",
       "    'serpapi_scholar_link': 'https://serpapi.com/search.json?as_sdt=5%2C33&cites=11671369688247503472&engine=google_scholar&hl=en'},\n",
       "   'related_pages_link': 'https://scholar.google.com/scholar?q=related:cApB1uQI-aEJ:scholar.google.com/&scioq=NLP&hl=en&as_sdt=0,33',\n",
       "   'serpapi_related_pages_link': 'https://serpapi.com/search.json?as_sdt=0%2C33&engine=google_scholar&hl=en&q=related%3AcApB1uQI-aEJ%3Ascholar.google.com%2F',\n",
       "   'versions': {'total': 19,\n",
       "    'link': 'https://scholar.google.com/scholar?cluster=11671369688247503472&hl=en&as_sdt=0,33',\n",
       "    'cluster_id': '11671369688247503472',\n",
       "    'serpapi_scholar_link': 'https://serpapi.com/search.json?as_sdt=0%2C33&cluster=11671369688247503472&engine=google_scholar&hl=en'}}},\n",
       " {'position': 3,\n",
       "  'title': 'Visualizing and understanding neural models in NLP',\n",
       "  'result_id': 'Ti8rqhtxTQQJ',\n",
       "  'link': 'https://arxiv.org/abs/1506.01066',\n",
       "  'snippet': '… how they achieve meaning composition in natural language processing. The next section describes some visualization models in vision and NLP that have inspired this work. We …',\n",
       "  'publication_info': {'summary': 'J Li, X Chen, E Hovy, D Jurafsky - arXiv preprint arXiv:1506.01066, 2015 - arxiv.org',\n",
       "   'authors': [{'name': 'J Li',\n",
       "     'link': 'https://scholar.google.com/citations?user=PwU16JEAAAAJ&hl=en&oi=sra',\n",
       "     'serpapi_scholar_link': 'https://serpapi.com/search.json?author_id=PwU16JEAAAAJ&engine=google_scholar_author&hl=en',\n",
       "     'author_id': 'PwU16JEAAAAJ'},\n",
       "    {'name': 'X Chen',\n",
       "     'link': 'https://scholar.google.com/citations?user=bSU7LYoAAAAJ&hl=en&oi=sra',\n",
       "     'serpapi_scholar_link': 'https://serpapi.com/search.json?author_id=bSU7LYoAAAAJ&engine=google_scholar_author&hl=en',\n",
       "     'author_id': 'bSU7LYoAAAAJ'},\n",
       "    {'name': 'E Hovy',\n",
       "     'link': 'https://scholar.google.com/citations?user=xMne5ZUAAAAJ&hl=en&oi=sra',\n",
       "     'serpapi_scholar_link': 'https://serpapi.com/search.json?author_id=xMne5ZUAAAAJ&engine=google_scholar_author&hl=en',\n",
       "     'author_id': 'xMne5ZUAAAAJ'},\n",
       "    {'name': 'D Jurafsky',\n",
       "     'link': 'https://scholar.google.com/citations?user=uZg9l58AAAAJ&hl=en&oi=sra',\n",
       "     'serpapi_scholar_link': 'https://serpapi.com/search.json?author_id=uZg9l58AAAAJ&engine=google_scholar_author&hl=en',\n",
       "     'author_id': 'uZg9l58AAAAJ'}]},\n",
       "  'resources': [{'title': 'arxiv.org',\n",
       "    'file_format': 'PDF',\n",
       "    'link': 'https://arxiv.org/pdf/1506.01066'}],\n",
       "  'inline_links': {'serpapi_cite_link': 'https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=Ti8rqhtxTQQJ',\n",
       "   'cited_by': {'total': 829,\n",
       "    'link': 'https://scholar.google.com/scholar?cites=310028312991444814&as_sdt=5,33&sciodt=0,33&hl=en',\n",
       "    'cites_id': '310028312991444814',\n",
       "    'serpapi_scholar_link': 'https://serpapi.com/search.json?as_sdt=5%2C33&cites=310028312991444814&engine=google_scholar&hl=en'},\n",
       "   'related_pages_link': 'https://scholar.google.com/scholar?q=related:Ti8rqhtxTQQJ:scholar.google.com/&scioq=NLP&hl=en&as_sdt=0,33',\n",
       "   'serpapi_related_pages_link': 'https://serpapi.com/search.json?as_sdt=0%2C33&engine=google_scholar&hl=en&q=related%3ATi8rqhtxTQQJ%3Ascholar.google.com%2F',\n",
       "   'versions': {'total': 17,\n",
       "    'link': 'https://scholar.google.com/scholar?cluster=310028312991444814&hl=en&as_sdt=0,33',\n",
       "    'cluster_id': '310028312991444814',\n",
       "    'serpapi_scholar_link': 'https://serpapi.com/search.json?as_sdt=0%2C33&cluster=310028312991444814&engine=google_scholar&hl=en'},\n",
       "   'cached_page_link': 'https://scholar.googleusercontent.com/scholar?q=cache:Ti8rqhtxTQQJ:scholar.google.com/+NLP&hl=en&as_sdt=0,33'}},\n",
       " {'position': 4,\n",
       "  'title': 'Natural language processing: an introduction',\n",
       "  'result_id': 'scyhLMaORyoJ',\n",
       "  'link': 'https://academic.oup.com/jamia/article-abstract/18/5/544/829676',\n",
       "  'snippet': '… evolution of NLP, and summarize common NLP sub-… We then provide a synopsis of selected highlights of medical NLP … for diverse NLP sub-problems, we discuss how modern NLP …',\n",
       "  'publication_info': {'summary': 'PM Nadkarni, L Ohno-Machado… - Journal of the …, 2011 - academic.oup.com',\n",
       "   'authors': [{'name': 'L Ohno-Machado',\n",
       "     'link': 'https://scholar.google.com/citations?user=2IvwHucAAAAJ&hl=en&oi=sra',\n",
       "     'serpapi_scholar_link': 'https://serpapi.com/search.json?author_id=2IvwHucAAAAJ&engine=google_scholar_author&hl=en',\n",
       "     'author_id': '2IvwHucAAAAJ'}]},\n",
       "  'resources': [{'title': 'oup.com',\n",
       "    'file_format': 'PDF',\n",
       "    'link': 'https://academic.oup.com/jamia/article-pdf/18/5/544/5962687/18-5-544.pdf'}],\n",
       "  'inline_links': {'serpapi_cite_link': 'https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=scyhLMaORyoJ',\n",
       "   'cited_by': {'total': 1845,\n",
       "    'link': 'https://scholar.google.com/scholar?cites=3046560654742899889&as_sdt=5,33&sciodt=0,33&hl=en',\n",
       "    'cites_id': '3046560654742899889',\n",
       "    'serpapi_scholar_link': 'https://serpapi.com/search.json?as_sdt=5%2C33&cites=3046560654742899889&engine=google_scholar&hl=en'},\n",
       "   'related_pages_link': 'https://scholar.google.com/scholar?q=related:scyhLMaORyoJ:scholar.google.com/&scioq=NLP&hl=en&as_sdt=0,33',\n",
       "   'serpapi_related_pages_link': 'https://serpapi.com/search.json?as_sdt=0%2C33&engine=google_scholar&hl=en&q=related%3AscyhLMaORyoJ%3Ascholar.google.com%2F',\n",
       "   'versions': {'total': 15,\n",
       "    'link': 'https://scholar.google.com/scholar?cluster=3046560654742899889&hl=en&as_sdt=0,33',\n",
       "    'cluster_id': '3046560654742899889',\n",
       "    'serpapi_scholar_link': 'https://serpapi.com/search.json?as_sdt=0%2C33&cluster=3046560654742899889&engine=google_scholar&hl=en'}}},\n",
       " {'position': 5,\n",
       "  'title': 'Progress in neural NLP: modeling, learning, and reasoning',\n",
       "  'result_id': 'EAQZYY4kZC8J',\n",
       "  'type': 'Html',\n",
       "  'link': 'https://www.sciencedirect.com/science/article/pii/S2095809919304928',\n",
       "  'snippet': '… of the neural NLP framework in three categories of efforts: ① neural NLP modeling aimed at designing appropriate network structures for different tasks; ② neural NLP learning aimed at …',\n",
       "  'publication_info': {'summary': 'M Zhou, N Duan, S Liu, HY Shum - Engineering, 2020 - Elsevier',\n",
       "   'authors': [{'name': 'M Zhou',\n",
       "     'link': 'https://scholar.google.com/citations?user=a0w5c0gAAAAJ&hl=en&oi=sra',\n",
       "     'serpapi_scholar_link': 'https://serpapi.com/search.json?author_id=a0w5c0gAAAAJ&engine=google_scholar_author&hl=en',\n",
       "     'author_id': 'a0w5c0gAAAAJ'},\n",
       "    {'name': 'N Duan',\n",
       "     'link': 'https://scholar.google.com/citations?user=Qaa6OxIAAAAJ&hl=en&oi=sra',\n",
       "     'serpapi_scholar_link': 'https://serpapi.com/search.json?author_id=Qaa6OxIAAAAJ&engine=google_scholar_author&hl=en',\n",
       "     'author_id': 'Qaa6OxIAAAAJ'},\n",
       "    {'name': 'S Liu',\n",
       "     'link': 'https://scholar.google.com/citations?user=6mNya-wAAAAJ&hl=en&oi=sra',\n",
       "     'serpapi_scholar_link': 'https://serpapi.com/search.json?author_id=6mNya-wAAAAJ&engine=google_scholar_author&hl=en',\n",
       "     'author_id': '6mNya-wAAAAJ'},\n",
       "    {'name': 'HY Shum',\n",
       "     'link': 'https://scholar.google.com/citations?user=9akH-n8AAAAJ&hl=en&oi=sra',\n",
       "     'serpapi_scholar_link': 'https://serpapi.com/search.json?author_id=9akH-n8AAAAJ&engine=google_scholar_author&hl=en',\n",
       "     'author_id': '9akH-n8AAAAJ'}]},\n",
       "  'resources': [{'title': 'sciencedirect.com',\n",
       "    'file_format': 'HTML',\n",
       "    'link': 'https://www.sciencedirect.com/science/article/pii/S2095809919304928'}],\n",
       "  'inline_links': {'serpapi_cite_link': 'https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=EAQZYY4kZC8J',\n",
       "   'html_version': 'https://www.sciencedirect.com/science/article/pii/S2095809919304928',\n",
       "   'cited_by': {'total': 218,\n",
       "    'link': 'https://scholar.google.com/scholar?cites=3414894611386663952&as_sdt=5,33&sciodt=0,33&hl=en',\n",
       "    'cites_id': '3414894611386663952',\n",
       "    'serpapi_scholar_link': 'https://serpapi.com/search.json?as_sdt=5%2C33&cites=3414894611386663952&engine=google_scholar&hl=en'},\n",
       "   'related_pages_link': 'https://scholar.google.com/scholar?q=related:EAQZYY4kZC8J:scholar.google.com/&scioq=NLP&hl=en&as_sdt=0,33',\n",
       "   'serpapi_related_pages_link': 'https://serpapi.com/search.json?as_sdt=0%2C33&engine=google_scholar&hl=en&q=related%3AEAQZYY4kZC8J%3Ascholar.google.com%2F',\n",
       "   'versions': {'total': 3,\n",
       "    'link': 'https://scholar.google.com/scholar?cluster=3414894611386663952&hl=en&as_sdt=0,33',\n",
       "    'cluster_id': '3414894611386663952',\n",
       "    'serpapi_scholar_link': 'https://serpapi.com/search.json?as_sdt=0%2C33&cluster=3414894611386663952&engine=google_scholar&hl=en'}}},\n",
       " {'position': 6,\n",
       "  'title': 'Principles of NLP: What it is, how it works',\n",
       "  'result_id': 'ISdnrjIY3CYJ',\n",
       "  'type': 'Book',\n",
       "  'link': 'https://books.google.com/books?hl=en&lr=&id=0dqrOf1Y91UC&oi=fnd&pg=PA2&dq=NLP&ots=usCnSIaYga&sig=cr5Ey72ZOSH-O7efcAM8R99oGhI',\n",
       "  'snippet': '… NLP presuppositions as their startingpoint, Joseph and Ian have emphasized the most generative aspect of NLP, and hopefully will help shift the focus of NLP … richness of NLP through …',\n",
       "  'publication_info': {'summary': \"J O'Connor, I McDermott - 2013 - books.google.com\"},\n",
       "  'inline_links': {'serpapi_cite_link': 'https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=ISdnrjIY3CYJ',\n",
       "   'cited_by': {'total': 134,\n",
       "    'link': 'https://scholar.google.com/scholar?cites=2800139674271033121&as_sdt=5,33&sciodt=0,33&hl=en',\n",
       "    'cites_id': '2800139674271033121',\n",
       "    'serpapi_scholar_link': 'https://serpapi.com/search.json?as_sdt=5%2C33&cites=2800139674271033121&engine=google_scholar&hl=en'},\n",
       "   'related_pages_link': 'https://scholar.google.com/scholar?q=related:ISdnrjIY3CYJ:scholar.google.com/&scioq=NLP&hl=en&as_sdt=0,33',\n",
       "   'serpapi_related_pages_link': 'https://serpapi.com/search.json?as_sdt=0%2C33&engine=google_scholar&hl=en&q=related%3AISdnrjIY3CYJ%3Ascholar.google.com%2F'}},\n",
       " {'position': 7,\n",
       "  'title': 'Integrating NLP using linked data',\n",
       "  'result_id': '6pL0VMMgMasJ',\n",
       "  'link': 'https://link.springer.com/chapter/10.1007/978-3-642-41338-4_7',\n",
       "  'snippet': '… We argue that simplifying the interoperability of different NLP tools performing similar but … of sophisticated NLP applications. In this paper, we present the NLP Interchange Format (NIF). …',\n",
       "  'publication_info': {'summary': 'S Hellmann, J Lehmann, S Auer… - The Semantic Web–ISWC …, 2013 - Springer',\n",
       "   'authors': [{'name': 'S Hellmann',\n",
       "     'link': 'https://scholar.google.com/citations?user=caLrIhoAAAAJ&hl=en&oi=sra',\n",
       "     'serpapi_scholar_link': 'https://serpapi.com/search.json?author_id=caLrIhoAAAAJ&engine=google_scholar_author&hl=en',\n",
       "     'author_id': 'caLrIhoAAAAJ'},\n",
       "    {'name': 'J Lehmann',\n",
       "     'link': 'https://scholar.google.com/citations?user=sEaQ5rgAAAAJ&hl=en&oi=sra',\n",
       "     'serpapi_scholar_link': 'https://serpapi.com/search.json?author_id=sEaQ5rgAAAAJ&engine=google_scholar_author&hl=en',\n",
       "     'author_id': 'sEaQ5rgAAAAJ'},\n",
       "    {'name': 'S Auer',\n",
       "     'link': 'https://scholar.google.com/citations?user=2cpal78AAAAJ&hl=en&oi=sra',\n",
       "     'serpapi_scholar_link': 'https://serpapi.com/search.json?author_id=2cpal78AAAAJ&engine=google_scholar_author&hl=en',\n",
       "     'author_id': '2cpal78AAAAJ'}]},\n",
       "  'resources': [{'title': 'aksw.org',\n",
       "    'file_format': 'PDF',\n",
       "    'link': 'http://svn.aksw.org/papers/2013/ISWC_NIF/public.pdf'}],\n",
       "  'inline_links': {'serpapi_cite_link': 'https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=6pL0VMMgMasJ',\n",
       "   'cited_by': {'total': 307,\n",
       "    'link': 'https://scholar.google.com/scholar?cites=12335676877660525290&as_sdt=5,33&sciodt=0,33&hl=en',\n",
       "    'cites_id': '12335676877660525290',\n",
       "    'serpapi_scholar_link': 'https://serpapi.com/search.json?as_sdt=5%2C33&cites=12335676877660525290&engine=google_scholar&hl=en'},\n",
       "   'related_pages_link': 'https://scholar.google.com/scholar?q=related:6pL0VMMgMasJ:scholar.google.com/&scioq=NLP&hl=en&as_sdt=0,33',\n",
       "   'serpapi_related_pages_link': 'https://serpapi.com/search.json?as_sdt=0%2C33&engine=google_scholar&hl=en&q=related%3A6pL0VMMgMasJ%3Ascholar.google.com%2F',\n",
       "   'versions': {'total': 11,\n",
       "    'link': 'https://scholar.google.com/scholar?cluster=12335676877660525290&hl=en&as_sdt=0,33',\n",
       "    'cluster_id': '12335676877660525290',\n",
       "    'serpapi_scholar_link': 'https://serpapi.com/search.json?as_sdt=0%2C33&cluster=12335676877660525290&engine=google_scholar&hl=en'}}},\n",
       " {'position': 8,\n",
       "  'title': 'ERASER: A benchmark to evaluate rationalized NLP models',\n",
       "  'result_id': 'k3bee8AuW-kJ',\n",
       "  'link': 'https://arxiv.org/abs/1911.03429',\n",
       "  'snippet': 'State-of-the-art models in NLP are now predominantly based … more interpretable deep models for NLP that reveal the `… on interpretable models in NLP. This benchmark comprises …',\n",
       "  'publication_info': {'summary': 'J DeYoung, S Jain, NF Rajani, E Lehman… - arXiv preprint arXiv …, 2019 - arxiv.org',\n",
       "   'authors': [{'name': 'J DeYoung',\n",
       "     'link': 'https://scholar.google.com/citations?user=f8aP6RMAAAAJ&hl=en&oi=sra',\n",
       "     'serpapi_scholar_link': 'https://serpapi.com/search.json?author_id=f8aP6RMAAAAJ&engine=google_scholar_author&hl=en',\n",
       "     'author_id': 'f8aP6RMAAAAJ'},\n",
       "    {'name': 'S Jain',\n",
       "     'link': 'https://scholar.google.com/citations?user=ng6th5cAAAAJ&hl=en&oi=sra',\n",
       "     'serpapi_scholar_link': 'https://serpapi.com/search.json?author_id=ng6th5cAAAAJ&engine=google_scholar_author&hl=en',\n",
       "     'author_id': 'ng6th5cAAAAJ'},\n",
       "    {'name': 'NF Rajani',\n",
       "     'link': 'https://scholar.google.com/citations?user=eIRG81YAAAAJ&hl=en&oi=sra',\n",
       "     'serpapi_scholar_link': 'https://serpapi.com/search.json?author_id=eIRG81YAAAAJ&engine=google_scholar_author&hl=en',\n",
       "     'author_id': 'eIRG81YAAAAJ'},\n",
       "    {'name': 'E Lehman',\n",
       "     'link': 'https://scholar.google.com/citations?user=dmmIl0IAAAAJ&hl=en&oi=sra',\n",
       "     'serpapi_scholar_link': 'https://serpapi.com/search.json?author_id=dmmIl0IAAAAJ&engine=google_scholar_author&hl=en',\n",
       "     'author_id': 'dmmIl0IAAAAJ'}]},\n",
       "  'resources': [{'title': 'arxiv.org',\n",
       "    'file_format': 'PDF',\n",
       "    'link': 'https://arxiv.org/pdf/1911.03429'}],\n",
       "  'inline_links': {'serpapi_cite_link': 'https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=k3bee8AuW-kJ',\n",
       "   'cited_by': {'total': 586,\n",
       "    'link': 'https://scholar.google.com/scholar?cites=16815085037964654227&as_sdt=5,33&sciodt=0,33&hl=en',\n",
       "    'cites_id': '16815085037964654227',\n",
       "    'serpapi_scholar_link': 'https://serpapi.com/search.json?as_sdt=5%2C33&cites=16815085037964654227&engine=google_scholar&hl=en'},\n",
       "   'related_pages_link': 'https://scholar.google.com/scholar?q=related:k3bee8AuW-kJ:scholar.google.com/&scioq=NLP&hl=en&as_sdt=0,33',\n",
       "   'serpapi_related_pages_link': 'https://serpapi.com/search.json?as_sdt=0%2C33&engine=google_scholar&hl=en&q=related%3Ak3bee8AuW-kJ%3Ascholar.google.com%2F',\n",
       "   'versions': {'total': 6,\n",
       "    'link': 'https://scholar.google.com/scholar?cluster=16815085037964654227&hl=en&as_sdt=0,33',\n",
       "    'cluster_id': '16815085037964654227',\n",
       "    'serpapi_scholar_link': 'https://serpapi.com/search.json?as_sdt=0%2C33&cluster=16815085037964654227&engine=google_scholar&hl=en'},\n",
       "   'cached_page_link': 'https://scholar.googleusercontent.com/scholar?q=cache:k3bee8AuW-kJ:scholar.google.com/+NLP&hl=en&as_sdt=0,33'}},\n",
       " {'position': 9,\n",
       "  'title': 'Tokenization as the initial phase in NLP',\n",
       "  'result_id': 'E3-R6rLsBjEJ',\n",
       "  'type': 'Pdf',\n",
       "  'link': 'https://aclanthology.org/C92-4173.pdf',\n",
       "  'snippet': 'In this paper, the authors address the significance and complexity of tokenization, the beginning step of NLP. Notions of word and token are discussed and defined from the viewpoints …',\n",
       "  'publication_info': {'summary': 'JJ Webster, C Kit - COLING 1992 volume 4: The 14th international …, 1992 - aclanthology.org',\n",
       "   'authors': [{'name': 'C Kit',\n",
       "     'link': 'https://scholar.google.com/citations?user=bJdKjIAAAAAJ&hl=en&oi=sra',\n",
       "     'serpapi_scholar_link': 'https://serpapi.com/search.json?author_id=bJdKjIAAAAAJ&engine=google_scholar_author&hl=en',\n",
       "     'author_id': 'bJdKjIAAAAAJ'}]},\n",
       "  'resources': [{'title': 'aclanthology.org',\n",
       "    'file_format': 'PDF',\n",
       "    'link': 'https://aclanthology.org/C92-4173.pdf'}],\n",
       "  'inline_links': {'serpapi_cite_link': 'https://serpapi.com/search.json?engine=google_scholar_cite&hl=en&q=E3-R6rLsBjEJ',\n",
       "   'cited_by': {'total': 560,\n",
       "    'link': 'https://scholar.google.com/scholar?cites=3532771210902470419&as_sdt=5,33&sciodt=0,33&hl=en',\n",
       "    'cites_id': '3532771210902470419',\n",
       "    'serpapi_scholar_link': 'https://serpapi.com/search.json?as_sdt=5%2C33&cites=3532771210902470419&engine=google_scholar&hl=en'},\n",
       "   'related_pages_link': 'https://scholar.google.com/scholar?q=related:E3-R6rLsBjEJ:scholar.google.com/&scioq=NLP&hl=en&as_sdt=0,33',\n",
       "   'serpapi_related_pages_link': 'https://serpapi.com/search.json?as_sdt=0%2C33&engine=google_scholar&hl=en&q=related%3AE3-R6rLsBjEJ%3Ascholar.google.com%2F',\n",
       "   'versions': {'total': 7,\n",
       "    'link': 'https://scholar.google.com/scholar?cluster=3532771210902470419&hl=en&as_sdt=0,33',\n",
       "    'cluster_id': '3532771210902470419',\n",
       "    'serpapi_scholar_link': 'https://serpapi.com/search.json?as_sdt=0%2C33&cluster=3532771210902470419&engine=google_scholar&hl=en'},\n",
       "   'cached_page_link': 'https://scholar.googleusercontent.com/scholar?q=cache:E3-R6rLsBjEJ:scholar.google.com/+NLP&hl=en&as_sdt=0,33'}}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "organic_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['**Title:** Double stimulations during the follicular and luteal phases of poor responders in IVF/ICSI programmes (Shanghai protocol)\\n**Link:** https://www.sciencedirect.com/science/article/pii/S1472648314004854\\n**Snippet:** … All rights are reserved, including those for text and data mining, AI training, and similar technologies. For all open access content, the Creative Commons licensing terms apply. …',\n",
       " '**Title:** Luteal-phase ovarian stimulation is feasible for producing competent oocytes in women undergoing in vitro fertilization/intracytoplasmic sperm injection …\\n**Link:** https://www.sciencedirect.com/science/article/pii/S0015028213030574\\n**Snippet:** Objective To explore the feasibility of luteal-phase ovarian stimulation using hMG and letrozole in terms of ovarian response and pregnancy outcome using frozen-thawed embryo …',\n",
       " \"**Title:** Where's the AI?\\n**Link:** https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/917\\n**Snippet:** … I survey four viewpoints about what AI is. I describe a program exhibiting AI as one that can … Because AI is a machine’s attempt to explain the behavior of the (human) system it is trying …\",\n",
       " '**Title:** Medroxyprogesterone acetate is an effective oral alternative for preventing premature luteinizing hormone surges in women undergoing controlled ovarian …\\n**Link:** https://www.sciencedirect.com/science/article/pii/S0015028215002277\\n**Snippet:** Objective To investigate the use of medroxyprogesterone acetate (MPA) to prevent LH surge during controlled ovarian hyperstimulation (COH) and to compare cycle characteristics and …',\n",
       " '**Title:** AI now report 2018\\n**Link:** https://www.stc.org/roundtable/wp-content/uploads/sites/34/2019/06/AI_Now_2018_Report.pdf\\n**Snippet:** … implications of AI technologies. It is the first university research center focused specifically on AI’s … and Meredith Whittaker, AI Now is one of the few women-led AI institutes in the world. …',\n",
       " '**Title:** AI now 2017 report\\n**Link:** https://experts.illinois.edu/en/publications/ai-now-2017-report\\n**Snippet:** Building on the inaugural 2016 report, the AI Now 2017 Report addresses the most recent scholarly literature in order to raise critical social questions that will shape our present and …',\n",
       " '**Title:** Comparison of live-birth defects after luteal-phase ovarian stimulation vs. conventional ovarian stimulation for in vitro fertilization and vitrified embryo transfer …\\n**Link:** https://www.sciencedirect.com/science/article/pii/S0015028215001508\\n**Snippet:** Objective To assess live-birth defects after a luteal-phase ovarian-stimulation regimen (LPS) for in vitro fertilization (IVF) and vitrified embryo transfer (ET) cycles. Design Retrospective …',\n",
       " '**Title:** A Review of Artificial Intelligence (AI) in Education from 2010 to 2020\\n**Link:** https://onlinelibrary.wiley.com/doi/abs/10.1155/2021/8812542\\n**Snippet:** … artificial intelligence (AI) has been applied to the education sector and explore the potential research trends and challenges of AI in … , as well as an assessment of AI in education, were …',\n",
       " '**Title:** Understanding and creating art with AI: Review and outlook\\n**Link:** https://dl.acm.org/doi/abs/10.1145/3475799\\n**Snippet:** … This article provides an integrated review of two facets of AI and art: (1) AI is used for art … to the role of AI in creating art, we address various practical and theoretical aspects of AI Art and …',\n",
       " '**Title:** What is AI, anyway?\\n**Link:** https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/623\\n**Snippet:** … goals of artificial intelligence, and a proposal of ten fundamental problems in AI research are … of the Yale AI technical reports In this context, examples of research conducted at the Yale …']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from serpapi import GoogleSearch\n",
    "\n",
    "def scholar_section(query):\n",
    "    if not query:\n",
    "        return \"No query provided.\"\n",
    "    \n",
    "    params = {\n",
    "        \"engine\": \"google_scholar\",\n",
    "        \"q\": query,\n",
    "        \"api_key\": \"c8b912a9727723424bffac813a03eb897d43cee8cfac0741c3b266a6cb8bef71\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        search = GoogleSearch(params)\n",
    "        results = search.get_dict()\n",
    "        organic_results = results.get(\"organic_results\", [])\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching results: {e}\")\n",
    "        return []\n",
    "    formatted_results = []\n",
    "    for result in organic_results:\n",
    "        title = result.get('title', 'No title')\n",
    "        link = result.get('link', 'No link')\n",
    "        snippet = result.get('snippet', 'No snippet')\n",
    "        \n",
    "        formatted_result = f\"**Title:** {title}\\n**Link:** {link}\\n**Snippet:** {snippet}\"\n",
    "        formatted_results.append(formatted_result)\n",
    "    \n",
    "    return formatted_results\n",
    "scholar_section(\"AI\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-12 02:17:04,751 - 7616 - __init__.py-__init__:531 - WARNING: Overriding of current TracerProvider is not allowed\n",
      "2024-08-12 02:17:05,490 - 7616 - __init__.py-__init__:531 - WARNING: Overriding of current TracerProvider is not allowed\n",
      "2024-08-12 02:17:06,206 - 7616 - __init__.py-__init__:531 - WARNING: Overriding of current TracerProvider is not allowed\n",
      "2024-08-12 02:17:06,964 - 7616 - __init__.py-__init__:531 - WARNING: Overriding of current TracerProvider is not allowed\n"
     ]
    }
   ],
   "source": [
    "from study_material import scholar_section\n",
    "from send_mess import send_message\n",
    "from fastapi import FastAPI, Request, Form\n",
    "from fastapi.responses import HTMLResponse\n",
    "from fastapi.staticfiles import StaticFiles\n",
    "from fastapi.templating import Jinja2Templates\n",
    "from crewai import Crew, Process\n",
    "from tools import serp_tool\n",
    "from agents import additional_information, question_generator, previous_history_generator, study_material\n",
    "from tasks import additional_information_task, question_generator_task, responses_question_generator_task, study_material_task\n",
    "import firebase_admin\n",
    "from firebase_admin import credentials, firestore\n",
    "from fpdf import FPDF\n",
    "import re\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Firebase initialization (unchanged)\n",
    "cred = credentials.Certificate(\"C:/PROJECTS/Quiz App/quizapp-7bc35-firebase-adminsdk-4denh-cb7f1c9dab.json\")\n",
    "#firebase_admin.initialize_app(cred, {\"databaseURL\": \"https://quizapp-7bc35-default-rtdb.firebaseio.com/\"})\n",
    "db = firestore.client()\n",
    "\n",
    "app.mount(\"/static\", StaticFiles(directory=\"static\"), name=\"static\")\n",
    "templates = Jinja2Templates(directory=\"templates\")\n",
    "\n",
    "crew_answers = Crew(\n",
    "    agents=[question_generator, additional_information],\n",
    "    tasks=[question_generator_task, additional_information_task],\n",
    "    process=Process.sequential,\n",
    "    memory=True,\n",
    "    cache=True,\n",
    "    max_rpm=100,\n",
    "    share_crew=True\n",
    ")\n",
    "\n",
    "crew_history_answers = Crew(\n",
    "    agents=[previous_history_generator, additional_information],\n",
    "    tasks=[responses_question_generator_task, additional_information_task],\n",
    "    process=Process.sequential,\n",
    "    memory=True,\n",
    "    cache=True,\n",
    "    max_rpm=100,\n",
    "    share_crew=True\n",
    ")\n",
    "\n",
    "crew_scholar = Crew(\n",
    "    agents=[study_material],\n",
    "    tasks=[study_material_task],\n",
    "    memory=True,\n",
    "    cache=True,\n",
    "    max_rpm=100,\n",
    "    share_crew=True\n",
    ")\n",
    "\n",
    "\n",
    "# app.py\n",
    "crew_information = Crew(\n",
    "    agents=[additional_information],\n",
    "    tasks=[additional_information_task],\n",
    "    process=Process.sequential,\n",
    "    memory=True,\n",
    "    cache=True,\n",
    "    max_rpm=100,\n",
    "    share_crew=True\n",
    ")\n",
    "\n",
    "def create_pdf(content, filename):\n",
    "    pdf = FPDF()\n",
    "    pdf.add_page()\n",
    "    pdf.set_font(\"Arial\", size=12)\n",
    "    \n",
    "    for line in content:\n",
    "        pdf.multi_cell(0, 10, line.encode('latin-1', 'replace').decode('latin-1'))\n",
    "        pdf.ln()\n",
    "    \n",
    "    pdf.output(filename)\n",
    "\n",
    "def parse_questions(questions_text):\n",
    "    question_pattern = re.compile(r\"\\*\\*Question \\d+:\\*\\*\\n\\n(.+?)\\n\\n\\(A\\) (.+?)\\n\\(B\\) (.+?)\\n\\(C\\) (.+?)\\n\\(D\\) (.+?)\\n\\nResult: (.+)\")\n",
    "    matches = question_pattern.findall(questions_text)\n",
    "    questions = []\n",
    "    for match in matches:\n",
    "        questions.append({\n",
    "            \"question\": match[0],\n",
    "            \"option_a\": match[1],\n",
    "            \"option_b\": match[2],\n",
    "            \"option_c\": match[3],\n",
    "            \"option_d\": match[4],\n",
    "            \"result\": match[5]\n",
    "        })\n",
    "    return questions\n",
    "\n",
    "\n",
    "def generate_questions_with_crew(topic, number_of_questions, level):\n",
    "    inputs = {'topic': topic, 'numberofquestions': number_of_questions, 'level': level}\n",
    "    try:\n",
    "        result = crew_answers.kickoff(inputs)\n",
    "        return result\n",
    "    except KeyError as e:\n",
    "        print(f\"KeyError: {e} - Check if all required keys are present in the inputs.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while generating questions: {e}\")\n",
    "\n",
    "def generate_questions_with_history(topic, number_of_questions, previous, level):\n",
    "    inputs = {'topic': topic, 'numberofquestions': number_of_questions, 'level': level, 'responses': previous}\n",
    "    try:\n",
    "        result = crew_history_answers.kickoff(inputs)\n",
    "        return result\n",
    "    except KeyError as e:\n",
    "        print(f\"KeyError: {e} - Check if all required keys are present in the inputs.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while generating questions: {e}\")\n",
    "\n",
    "def provide_question_information(question):\n",
    "    inputs = {'question': question}\n",
    "    \n",
    "    result = crew_answers.kickoff(inputs)\n",
    "    return result\n",
    "    \n",
    "\n",
    "def fetch_study_material(topic):\n",
    "    inputs = {'topic': topic}\n",
    "    try:\n",
    "        result = crew_scholar.kickoff(inputs)\n",
    "        return result\n",
    "    except KeyError as e:\n",
    "        print(f\"KeyError: {e} - Check if all required keys are present in the inputs.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while fetching study material: {e}\")\n",
    "\n",
    "@app.get(\"/\", response_class=HTMLResponse)\n",
    "async def read_root(request: Request):\n",
    "    return templates.TemplateResponse(\"index.html\", {\"request\": request})\n",
    "\n",
    "@app.get(\"/leaderboard\", response_class=HTMLResponse)\n",
    "async def get_leaderboard(request: Request):\n",
    "    try:\n",
    "        collection_ref = db.collection('user_responses')\n",
    "        docs = collection_ref.stream()\n",
    "        documents = []\n",
    "        for doc in docs:\n",
    "            data = doc.to_dict()\n",
    "            print(f'Document Data: {data}') \n",
    "            documents.append({\n",
    "                'id': doc.id,\n",
    "                'name': data.get('name', 'N/A'),\n",
    "                'score': data.get('score', 0),\n",
    "                'topic': data.get('topic', 'N/A')  \n",
    "            })\n",
    "        \n",
    "        documents.sort(key=lambda x: x['score'], reverse=True)\n",
    "    \n",
    "        return templates.TemplateResponse(\"leaderboard.html\", {\"request\": request, \"leaderboard\": documents})\n",
    "    \n",
    "    except Exception as e:\n",
    "        return templates.TemplateResponse(\"error.html\", {\"request\": request, \"error\": str(e)})\n",
    "\n",
    "@app.post(\"/generate_quiz\", response_class=HTMLResponse)\n",
    "async def generate_quiz(request: Request, name: str = Form(...), email: str = Form(...), topic: str = Form(...), num_questions: int = Form(...), level: str = Form(...)):\n",
    "    try:\n",
    "        print(f\"Received topic: {topic}, level: {level}\")\n",
    "        \n",
    "        existing_user = db.collection(\"user_responses\").where(\"email\", \"==\", email).get()\n",
    "        \n",
    "        if existing_user:\n",
    "            previous_responses = []\n",
    "            for user in existing_user:\n",
    "                user_data = user.to_dict()\n",
    "                if \"responses\" in user_data:\n",
    "                    previous_responses.extend(user_data[\"responses\"])\n",
    "            previous_responses_str = ', '.join([f\"{response['question']}: {response['user_answer']}\" for response in previous_responses])\n",
    "            questions_text = generate_questions_with_history(topic, num_questions, previous_responses_str, level)\n",
    "        else:\n",
    "            questions_text = generate_questions_with_crew(topic, num_questions, level)\n",
    "        \n",
    "        questions = parse_questions(questions_text)\n",
    "        \n",
    "        return templates.TemplateResponse(\"quiz.html\", {\n",
    "            \"request\": request,\n",
    "            \"name\": name,\n",
    "            \"email\": email,\n",
    "            \"topic\": topic,\n",
    "            \"num_questions\": num_questions,\n",
    "            \"questions\": questions\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating quiz: {e}\")\n",
    "        return templates.TemplateResponse(\"error.html\", {\"request\": request, \"error\": str(e)})\n",
    "\n",
    "@app.post(\"/submit_quiz\", response_class=HTMLResponse)\n",
    "async def submit_quiz(request: Request):\n",
    "    try:\n",
    "        form = await request.form()\n",
    "        \n",
    "        name = form.get(\"name\", \"\").strip()\n",
    "        email = form.get(\"email\", \"\").strip()\n",
    "        topic = form.get(\"topic\", \"\").strip()\n",
    "        print(f\"Topic is: {topic}\")\n",
    "        \n",
    "        question_keys = [key for key in form.keys() if key.startswith(\"question_\")]\n",
    "        total_questions = len(question_keys)\n",
    "        score = 0\n",
    "        \n",
    "        questions = []\n",
    "        \n",
    "        for i in range(total_questions):\n",
    "            question_key = f\"question_{i}\"\n",
    "            user_answer = form.get(question_key, \"\").strip()\n",
    "            \n",
    "            if not user_answer:\n",
    "                continue\n",
    "            \n",
    "            correct_answer = form.get(f\"correct_answer_{i}\", \"\").strip()\n",
    "            question_text = form.get(f\"question_text_{i}\", \"\").strip()\n",
    "            option_a = form.get(f\"option_a_{i}\", \"\").strip()\n",
    "            option_b = form.get(f\"option_b_{i}\", \"\").strip()\n",
    "            option_c = form.get(f\"option_c_{i}\", \"\").strip()\n",
    "            option_d = form.get(f\"option_d_{i}\", \"\").strip()\n",
    "            correct_answer = correct_answer.split(\":\")[0].strip() \n",
    "            correct_answer = correct_answer[0]\n",
    "            if question_text:\n",
    "                questions.append({\n",
    "                    \"question\": question_text,\n",
    "                    \"option_a\": option_a,\n",
    "                    \"option_b\": option_b,\n",
    "                    \"option_c\": option_c,\n",
    "                    \"option_d\": option_d,\n",
    "                    \"user_answer\": user_answer,\n",
    "                    \"result\": correct_answer,\n",
    "                    \"additional_info\": provide_question_information(question_text) if question_text else \"\"\n",
    "                })\n",
    "            \n",
    "                if user_answer == correct_answer:\n",
    "                    score += 10\n",
    "        \n",
    "        max_score = len(questions) * 10\n",
    "        \n",
    "        user_data = {\n",
    "            \"name\": name,\n",
    "            \"email\": email,\n",
    "            \"responses\": [{\n",
    "                \"question\": q[\"question\"],\n",
    "                \"user_answer\": q[\"user_answer\"],\n",
    "                \"correct_answer\": q[\"result\"]\n",
    "            } for q in questions],\n",
    "            \"score\": score\n",
    "        }\n",
    "        db.collection(\"user_responses\").add(user_data)\n",
    "\n",
    "        study_material = []\n",
    "        if topic:\n",
    "            study_material = fetch_study_material(topic)\n",
    "            if not study_material or \"No results found.\" in study_material:\n",
    "                study_material = [\"No study material found for this topic.\"]\n",
    "            else:\n",
    "                print(f\"Study material for topic '{topic}' fetched successfully.\")\n",
    "        else:\n",
    "            print(\"No valid topic provided, skipping study material retrieval.\")\n",
    "            study_material = [\"No topic was provided, so no study material could be retrieved.\"]\n",
    "        \n",
    "        create_pdf(study_material, \"study_material.pdf\")\n",
    "        create_pdf([f\"Your score: {score}/{max_score}\"] + [\n",
    "            f\"Question: {q['question']}\\nOptions:\\n(A) {q['option_a']}\\n(B) {q['option_b']}\\n(C) {q['option_c']}\\n(D) {q['option_d']}\\nYour Answer: {q['user_answer']}\\nCorrect Answer: {q['result']}\\nAdditional Info: {q['additional_info']}\"\n",
    "            for q in questions], \"quiz_result.pdf\")\n",
    "\n",
    "        send_message(name, email, \"quiz_result.pdf\", \"study_material.pdf\")\n",
    "        \n",
    "        return templates.TemplateResponse(\"quiz_result.html\", {\n",
    "            \"request\": request,\n",
    "            \"name\": name,\n",
    "            \"email\": email,\n",
    "            \"questions\": questions,\n",
    "            \"score\": score,\n",
    "            \"total_questions\": len(questions),\n",
    "            \"max_score\": max_score\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Error submitting quiz: {e}\")\n",
    "        return templates.TemplateResponse(\"error.html\", {\"request\": request, \"error\": str(e)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_ollamaNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading langchain_ollama-0.1.1-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.20 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain_ollama) (0.2.29)\n",
      "Collecting ollama<1,>=0.3.0 (from langchain_ollama)\n",
      "  Downloading ollama-0.3.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.20->langchain_ollama) (6.0.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.20->langchain_ollama) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.75 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.20->langchain_ollama) (0.1.81)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.20->langchain_ollama) (23.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.20->langchain_ollama) (2.8.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.20->langchain_ollama) (8.2.3)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.20->langchain_ollama) (4.11.0)\n",
      "Requirement already satisfied: httpx<0.28.0,>=0.27.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ollama<1,>=0.3.0->langchain_ollama) (0.27.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ollama<1,>=0.3.0->langchain_ollama) (4.3.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ollama<1,>=0.3.0->langchain_ollama) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ollama<1,>=0.3.0->langchain_ollama) (1.0.5)\n",
      "Requirement already satisfied: idna in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ollama<1,>=0.3.0->langchain_ollama) (2.10)\n",
      "Requirement already satisfied: sniffio in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ollama<1,>=0.3.0->langchain_ollama) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->ollama<1,>=0.3.0->langchain_ollama) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.20->langchain_ollama) (2.4)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.20->langchain_ollama) (3.10.3)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.20->langchain_ollama) (2.31.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3,>=1->langchain-core<0.3.0,>=0.2.20->langchain_ollama) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3,>=1->langchain-core<0.3.0,>=0.2.20->langchain_ollama) (2.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.20->langchain_ollama) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.20->langchain_ollama) (2.2.2)\n",
      "Downloading langchain_ollama-0.1.1-py3-none-any.whl (12 kB)\n",
      "Downloading ollama-0.3.1-py3-none-any.whl (10 kB)\n",
      "Installing collected packages: ollama, langchain_ollama\n",
      "Successfully installed langchain_ollama-0.1.1 ollama-0.3.1\n"
     ]
    }
   ],
   "source": [
    "pip install langchain_ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-12 02:32:06,320 - 7616 - __init__.py-__init__:531 - WARNING: Overriding of current TracerProvider is not allowed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred while providing question information: Error code: 401 - {'error': {'message': 'Incorrect API key provided: LL-cFNY3*******************************************************jlb7. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from fastapi import FastAPI\n",
    "from crewai import Crew, Process, Agent, Task\n",
    "from crewai_tools import SerperDevTool\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Ensure the API key is set correctly in the environment variables\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not google_api_key:\n",
    "    raise ValueError(\"API key for Google Generative AI is not set. Please set it in the .env file.\")\n",
    "\n",
    "# Initialize LLM with the correct API key\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-pro\",\n",
    "    verbose=True,\n",
    "    temperature=0.5,\n",
    "    google_api_key=google_api_key\n",
    ")\n",
    "\n",
    "# Define the additional_information agent\n",
    "additional_information = Agent(\n",
    "    role=\"Information Provider\",\n",
    "    goal=\"Based on the question: {question}, provide all necessary information to answer it in a summarized form.\",\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    memory=True,\n",
    "    backstory=(\n",
    "        \"By carefully analyzing the question, provide a summarized view of the necessary information.\"\n",
    "    ),\n",
    "    tools=[]\n",
    ")\n",
    "\n",
    "# Define the additional_information_task\n",
    "additional_information_task = Task(\n",
    "    description=\"Provide necessary background information for answering specific questions related to a given question.\",\n",
    "    expected_output=\"A summary of relevant information needed to answer the question.\",\n",
    "    agent=additional_information,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Configure the crew to handle additional_information_task\n",
    "crew_information = Crew(\n",
    "    agents=[additional_information],\n",
    "    tasks=[additional_information_task],\n",
    "    process=Process.sequential,\n",
    "    memory=True,\n",
    "    cache=True,\n",
    "    max_rpm=100,\n",
    "    share_crew=True\n",
    ")\n",
    "\n",
    "# Define the function to provide question information\n",
    "def provide_question_information(question):\n",
    "    inputs = {'question': question}\n",
    "    try:\n",
    "        result = crew_information.kickoff(inputs)\n",
    "        return result\n",
    "    except KeyError as e:\n",
    "        print(f\"KeyError: {e} - Check if all required keys are present in the inputs.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while providing question information: {e}\")\n",
    "\n",
    "# Test the function\n",
    "result = provide_question_information(\"What is the nature of the light?\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.37.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai) (4.3.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai) (2.8.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (2.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-12 13:42:04,917 - 12960 - __init__.py-__init__:531 - WARNING: Overriding of current TracerProvider is not allowed\n",
      "2024-08-12 13:42:04,921 - 12960 - __init__.py-__init__:531 - WARNING: Overriding of current TracerProvider is not allowed\n",
      "2024-08-12 13:42:04,925 - 12960 - __init__.py-__init__:531 - WARNING: Overriding of current TracerProvider is not allowed\n",
      "2024-08-12 13:42:04,927 - 12960 - __init__.py-__init__:531 - WARNING: Overriding of current TracerProvider is not allowed\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The default Firebase app already exists. This means you called initialize_app() more than once without providing an app name as the second argument. In most cases you only need to call initialize_app() once. But if you do want to initialize multiple apps, pass a second argument to initialize_app() to give each app a unique name.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 39\u001b[0m\n\u001b[0;32m     33\u001b[0m crew_neccessary_information \u001b[38;5;241m=\u001b[39m Crew(\n\u001b[0;32m     34\u001b[0m     agents\u001b[38;5;241m=\u001b[39m [additional_information],\n\u001b[0;32m     35\u001b[0m     tasks\u001b[38;5;241m=\u001b[39m [additional_information_task]\n\u001b[0;32m     36\u001b[0m )\n\u001b[0;32m     38\u001b[0m cred \u001b[38;5;241m=\u001b[39m credentials\u001b[38;5;241m.\u001b[39mCertificate(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/PROJECTS/Quiz App/quizapp-7bc35-firebase-adminsdk-4denh-cb7f1c9dab.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 39\u001b[0m \u001b[43mfirebase_admin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitialize_app\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdatabaseURL\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttps://quizapp-7bc35-default-rtdb.firebaseio.com/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m db \u001b[38;5;241m=\u001b[39m firestore\u001b[38;5;241m.\u001b[39mclient()\n\u001b[0;32m     43\u001b[0m app\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/static\u001b[39m\u001b[38;5;124m\"\u001b[39m, StaticFiles(directory\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatic\u001b[39m\u001b[38;5;124m\"\u001b[39m), name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatic\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\firebase_admin\\__init__.py:73\u001b[0m, in \u001b[0;36minitialize_app\u001b[1;34m(credential, options, name)\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m app\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m _DEFAULT_APP_NAME:\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m((\n\u001b[0;32m     74\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe default Firebase app already exists. This means you called \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     75\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minitialize_app() more than once without providing an app name as \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     76\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe second argument. In most cases you only need to call \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     77\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minitialize_app() once. But if you do want to initialize multiple \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     78\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapps, pass a second argument to initialize_app() to give each app \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     79\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma unique name.\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m((\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFirebase app named \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m already exists. This means you called \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minitialize_app() more than once with the same app name as the \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msecond argument. Make sure you provide a unique name every time \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myou call initialize_app().\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mformat(name))\n",
      "\u001b[1;31mValueError\u001b[0m: The default Firebase app already exists. This means you called initialize_app() more than once without providing an app name as the second argument. In most cases you only need to call initialize_app() once. But if you do want to initialize multiple apps, pass a second argument to initialize_app() to give each app a unique name."
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "# Directly set the API key\n",
    "api_key = \"AIzaSyBVGVq4nGLsndbVQmlaxdmz4WDcRnSFCBA\"  # Replace this with your actual API key\n",
    "\n",
    "# Ensure the API key is set\n",
    "if not api_key:\n",
    "    raise ValueError(\"API key for Google Generative AI is not set. Please set it in the script.\")\n",
    "\n",
    "# Set the API key\n",
    "openai.api_key = api_key\n",
    "\n",
    "try:\n",
    "    response = openai.Completion.create(\n",
    "        model=\"text-davinci-003\",\n",
    "        prompt=\"What is the nature of light?\",\n",
    "        max_tokens=50\n",
    "    )\n",
    "    print(\"API Key is valid. Response from OpenAI API:\")\n",
    "    print(response.choices[0].text.strip())\n",
    "except openai.error.AuthenticationError:\n",
    "    print(\"Authentication error: Invalid API key provided.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyError: 'question' - Check if all required keys are present in the inputs.\n"
     ]
    }
   ],
   "source": [
    "generate_questions_with_crew(\"AIML\", 6 , \"EASY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyError: 'topic' - Check if all required keys are present in the inputs.\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "result = provide_question_information(\"What is the nature of the light?\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred while fetching study material: Error code: 401 - {'error': {'message': 'Incorrect API key provided: fake. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}\n"
     ]
    }
   ],
   "source": [
    "fetch_study_material(\"AI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyError: 'topic' - Check if all required keys are present in the inputs.\n"
     ]
    }
   ],
   "source": [
    "provide_question_information(\"What is the nature of the light?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "def generate_ingredients(food):\n",
    "    prompt=\"\"\"You are Youtube video summarizer. You will be taking the transcript text\n",
    "    and summarizing the entire video and providing the important summary in points\n",
    "    within 1000 words. Please provide the summary of the text given here:  \"\"\"\n",
    "    model=genai.GenerativeModel(\"gemini-pro\")\n",
    "    response=model.generate_content(prompt+transcript_text)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
